\documentclass[12pt]{article}
\usepackage{geometry}
\usepackage{amsfonts, epsfig}
\usepackage{amsmath}
\usepackage{wrapfig} % for wrapping text around figures and tables
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{booktabs} % for better table rules
\usepackage{caption} % to customize caption format
\usepackage{linegoal} % for determining the remaining width of the line
\usepackage[backend=biber, sorting=none]{biblatex}
\usepackage{subcaption}

% reference file
\addbibresource{report.bib}

\geometry{
    left=20mm,
    right=20mm,
    top=20mm,
    bottom=20mm
}

% header and footer
\makeatletter
\def\@oddhead{\parbox{\textwidth}{\raggedright\sffamily\underline{Question 1 - Visualization and analysis of the Palmer penguin dataset} \hfill \thepage}}
\def\@oddfoot{\parbox{\textwidth}{\raggedright\footnotesize\texttt{emamtm0067.github.io} / \texttt{ematm0044.github.io}}}
\makeatother

% Redefine the table and figure formats to bold
\captionsetup[table]{labelfont=bf}
%\captionsetup[table]{labelfont=bf, justification=centering,skip=-2pt}
% \captionsetup[figure]{labelfont=bf}
\captionsetup[figure]{labelfont=bf, justification=centering}
\captionsetup{font=small}

\begin{document}
\begin{center}
\subsection*{Q1 - Visualization and analysis of the Palmer dataset}
\end{center}

\begin{wraptable}{r}{0.6\textwidth} % {alignment}{width}
  \small
  \begin{center}
  \vspace{-1.5\baselineskip} % Remove space before the table
  \setlength{\abovecaptionskip}{5pt}
  \setlength{\belowcaptionskip}{5pt}
  \fontsize{10}{10}\selectfont % Change font size here
  \begin{tabular}{l|l|l|l}
  % \begin{tabular}{|p{1.8cm}|p{1.2cm}|p{4cm}|p{1.8cm}|}
  \textbf{Feature}&\textbf{Type}&\textbf{Values in the dataset}&\textbf{Importance}\\
  \hline
  island&categorial&Torgersen, Biscoe, Dream&\ \ \ 0.12 (4)\\
  bill length&numerical&32.1mm - 59.6mm&\ \ \ 0.37 (1)\\
  bill depth&numerical&13.1mm - 21.5mm&\ \ \ 0.17 (3)\\
  flipper length&numerical&172mm - 231mm&\ \ \ 0.23 (2)\\
  body mass&numerical&2700g - 6300g&\ \ \ 0.11 (5)\\
  sex&categorial&Male, Female&\ \ \ 0.01 (6)\\
  species&categorial&Adelie, Chinstrap, Gentoo&\ \ \ \ class\\
  \end{tabular}
  \vspace{-2\baselineskip} % Remove space before title
  \end{center} 
  \caption{\centering\linespread{0.8}\selectfont Palmer penguin dataset features. Importance was calculated using random forest and a ranking is shown.}
  \vspace{-1\baselineskip} % Remove space after the table
  \label{tab:dataset}
\end{wraptable} 

\noindent
The Palmer penguin dataset consists of 344 records of the physical attributes of three species of penguin 
living on three islands in Antarctica (Table~\ref{tab:dataset}) \cite{PM}. 
In this report, consideration is given to data cleaning and preparation, 
the dataset is explored through visualization and analysis is carried out 
to compare the accuracy performances of a small number of AI approaches. 

\vspace{\baselineskip}
\subsection*{Data cleaning - missing values, encoding, standardization and imbalance}

\begin{wrapfigure}{r}{0.5\textwidth} % {alignment}{width}
  \centering
  \vspace{-1.5\baselineskip} % Remove space before the table
  \includegraphics[width=0.48\textwidth]{sex.png} % Adjust the width as needed
  \vspace{-0.5\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont All numerical features show a significant statistical difference between male and female, 
  as in the body mass above. Shown are median values, upper and lower quartiles, 
  and outliers.}
  \vspace{-0.5\baselineskip} % Remove space before title
  \label{fig:sex}
\end{wrapfigure}   

The two records missing the sex amd all numerical features were removed as imputation is unlikely to be reliable. 
The remaining nine records are missing only the sex attribute. Figure~\ref{fig:sex} shows the physical 
attributes of the male and female of each species differ statistically and so it is reasonable to 
consider imputing sex for those records. Following standardization, 
a Shapiro-Wilk test confirmed each numerical attribute has a normal distribution \cite{shapiro1965analysis} 
and separate Z-tests were applied to assess the 
hypotheses that the missing sex value is male or female \cite{freedman2007statistics}. 
Two of the records could be imputed as male and three as female 
and these were retained. with the remaining four records being removed. 
The cleaned dataset has 338 records, 147 Adelie  
(74 male, 73 female), 68 Chinstrap (34 male, 34 female) and 123 Gentoo (62 male, 61 female).

The categorical features in the dataset were encoded to numerical values. `One-shot' encoding of 
categorical features was not found to 
improve performance. A number of AI methods are known to be biased in favour of numerical features 
with smaller standard deviations \cite{hastie2009elements}, but this can be reduced by standardization 
of features to zero mean and unity standard deviation. 
Standardization statistics are calculated only from the training set, but are also applied to the test set. 
If a dataset is imbalanced, 
AI predictions may be biased towards classes more frequently found in the training data. 
In the Palmer penguin dataset, the number of Chinstrap records is around half of that of both Adelie and Gentoo, 
but, as all the methods adopted in the current work are known to be little affected by 
imbalanced data \cite{he2009learning}, no modifications were made.

% \vspace{\baselineskip}
\subsection*{Visualization of the dataset}

Figure~\ref{fig:islands} shows the species distribution for the islands. 
Chinstrap and Gentoo penguins are found only on one island, making island a potential confounding factor whose 
individual environmental factors may influence physical characteristics. 
A Shapiro-Wilk test confirmed the normal distribution of the numerical features of the Adelie penguins (found on 
all islands) and an ANOVA test confirmed the features are not significantly influenced by the island inhabited. 
Consequently, the island is not a confounding factor in the dataset.

\begin{wrapfigure}{R}{0.5\textwidth} % {alignment}{width}
  \centering
  \vspace{-1.5\baselineskip} % Remove space before the table
  \includegraphics[width=0.48\textwidth]{islands.png} % Adjust the width as needed
  \vspace{-0.5\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont Adelie samples are from all three islands, but Gentoo and Chinstrap from only one}
  \vspace{-1.5\baselineskip} % Remove space after title
  \label{fig:islands}
\end{wrapfigure}

The feature importance scores in Table~\ref{tab:dataset} represent 
relative contributions in predicting the species and were calculated using a random forest approach.
Results reported later show that performance improvements can be achieved by
concentrating classification on the more important features.

The pairwise scatterplots for the numerical features are shown in Figure~\ref{fig:pairwise}. 
Bill depth, in combination with either flipper length or body mass, 
yields a separable cluster of Gentoo penguins (shown in green) allowing them to be identified. 
No pairwise combination completely separates Adelie (orange) from Chinstrap (purple) clusters, 
but the best candidate feature for doing so is in the distributions involving bill length.

Figure~\ref{fig:sex} above shows there is a difference in the body masses of the male and female samples for each of the three species. 
Differences between the sexes for the other three numerical physical characteristics in the dataset were also apparent. 
Since narrower distributions are apparent if the sex of the species is considered rather than just the species itself, 
including sex is likely to provide a finer grained distinction for species classification 
and this knowledge can be used to improve performance, as discussed in the analysis section below. 

\subsection*{Methodology}

\begin{wrapfigure}{r}{0.6\textwidth} % {alignment}{width}
  \centering
  \vspace{-3\baselineskip} % Remove space before the figure
  \includegraphics[width=0.58\textwidth]{pairwise.png} % Adjust the width as needed
  \vspace{-0.5\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont Pairwise distributions of numerical features. Gentoo can be distinguished, 
  but Adelie and Chinstrap may not be completely separable from one another}
  \vspace{-0.5\baselineskip} % Remove space before title
  \label{fig:pairwise}
\end{wrapfigure}

All code is in Python 3.11 \cite{python311} using ‘Scikit-Learn’ libraries \cite{scikit-learn} 
under Ubuntu Linux \cite{ubuntu}. The code is available in a Gitub repository \cite{TimAIRepo}. 
Predicting the penguin species from the given features is a classification problem. 
Results are obtained from a baseline method, two convention classification approaches, 
namely \textit{k}-Nearest Neighbour (\textit{k}nn) \cite{bishop2006pattern} and random forest \cite{breiman2001random}, 
unsupervised \textit{k}-means (following cluster labelling) \cite{tan2005introduction} 
and a novel combined visualization and analysis (CVA) approach that uses 
practical visualizations and Support Vector Machine (SVM) classification.

To reduce the potential for overfitting, the classification methods (all but \textit{k}-means) were trained using 
`holdout validation', where 80\% of the dataset was used in a five-fold cross-validation 
configuration \cite{james2013introduction}. The remaining 20\% was kept for a test set. For all methods, 
the Scikit-Learn function GridSearchCV was employed to tune metaparameters \cite{scikit-learn}. 
Table~\ref{tab:metaparameters} shows the values selected for the metaparameter grid. Those giving the best 
performance were selected to generate accuracy results (the percentage of correctly predicted species) 
from the test set. Calculation of the metrics `precision' and `recall' were also considered, 
but these are only relevant if false positives or false negatives (respectively) are to be avoided.   

\subsection*{Results and analysis}

\begin{wraptable}{r}{0.55\textwidth} % {alignment}{width}
  \small
  \begin{center}
  \vspace{-4\baselineskip} % Remove space before the table
  \setlength{\abovecaptionskip}{5pt}
  \setlength{\belowcaptionskip}{5pt}
  \fontsize{10}{10}\selectfont % Change font size here
  \begin{tabular}{l|l|l}
  Method & Metaparameters & Values considered\\
  \hline
  \multirow{3}{*}{\textit{k}nn}    & nearest neighbours \textit{k}	  & \textit{\textbf{1}}, 2, 3, 4, 5, 6, 8, 10 \\
                                   & prediction weight function	& \textit{\textbf{uniform}}, distance \\
                                   & neighbours distance metric 	& \textbf{Manhattan}, Euclidean \\
  \hline
  \multirow{6}{*}[0.5ex]{\begin{tabular}[t]{@{}l@{}}random \\ forest\end{tabular}} 
                                  & trees in the forest & 5, \textit{\textbf{10}}, 15, 20, 25 \\
                                  & maximum depth of trees & \textit{\textbf{no maximum}}, 10, 20 \\
                                  & min samples to split node & \textit{\textbf{2}}, 5, 10 \\
                                  & min samples at leaf node & \textit{\textbf{1}}, 2, 4 \\
                                  & quality of split function & \textit{\textbf{gini}}, entropy \\

  \hline
  \multirow{4}{*}{\textit{k}-means} & number of clusters \textit{k}  & 2, \textit{\textbf{3}}, 4, 5, 6, 7, 8, 9, 10 \\
                                    & centroid initialization         & \textit{\textbf{k-means++}}, random \\
                                    & runs for centroid seeds	     & 2, \textit{\textbf{5}}, 10, 20 \\
                                    & max number of iterations           & 5, \textit{\textbf{10}}, 20, 50 \\
  \hline
    \multirow{3}{*}{CVA}            & regularization parameter  & 0.1, 1, \textit{\textbf{10}}, 100 \\
                                    & kernel coefficient      & \textit{\textbf{1}}, 0.1, 0.01, 0.001 \\
                                    & kernel type                  & rbf, \textit{\textbf{linear}}, polynomial \\
  \hline
  \end{tabular}
  \vspace{-1.5\baselineskip} % Remove space before title
  \end{center} 
  \caption{\centering\linespread{0.8}\selectfont Metaparameters values shown in italics most consistently produced training results of 
  best accuracy during validation and were selected for generating results}
  \vspace{-1\baselineskip} % Remove space after the table
  \label{tab:metaparameters}
\end{wraptable} 

Scikit-Learn provides pseudo-random procedures for selecting validation and test set values and 100 of these were used 
both when selecting metaparameters and when deriving accuracy results.

The results in Table~\ref{tab:results} include a baseline that is used to demonstrate performance improvements achieved by the AI methods being considered. 
In classification, the baseline method is often simply to select the most frequent class in the observations and, 
in this work, this is the Adelie penguins, giving an accuracy of 43.49\% (147/338).

\begin{wraptable}{r}{0.4\textwidth} % {alignment}{width}
  \small
  \begin{center}
  \vspace{-3\baselineskip} % Remove space before the table
  \setlength{\abovecaptionskip}{5pt}
  \setlength{\belowcaptionskip}{5pt}
  \fontsize{10}{10}\selectfont % Change font size here
  \begin{tabular}{l|l}
  Method & Accuracy\\
  \hline
  baseline, most numerous species & 43.49\% \\
  \hline
  \textit{k}NN, all features	& 99.24\% \\
  \textit{k}NN, no island &	99.46\% \\
  \hline
  random forest, all features	& 98.75\% \\
  random forest, three main features	& 98.57\% \\
  \hline
  \textit{k}-means, numerical features & 97.06\% \\
  \textit{k}-means, two sex clusters  & 98.23\% \\
  \hline
  CVA, three main features & 98.56\% \\
  CVA, as above but with sex & 98.78\% \\
  \hline
  \end{tabular}
  \vspace{-2\baselineskip} % Remove space before title
  \end{center} 
  \caption{\centering\linespread{0.8}\selectfont Mean classification accuracy from 100 test sets each generated by a pseudo random approach 
  and using the metaparameters identified in Table~\ref{tab:metaparameters}}
  \vspace{-1\baselineskip} % Remove space after the table
  \label{tab:results}
\end{wraptable} 

\textbf{Classification method 1 - \textit{k}nn}  
The performance of \textit{k}nn was found to be improved by omitting features from training. 
An exhaustive search involving omitting all combinations of features in turn determined that the best accuracy was obtained 
when island was omitted and when \textit{k}=3. It appears that island was not providing any additional information 
and the higher value of \textit{k} implies better generalization may have been achieved.

\begin{wrapfigure}{r}{0.46\textwidth} % {alignment}{width}
  \centering
  \vspace{-2\baselineskip} % Remove space before the figure
  \includegraphics[width=0.48\textwidth]{kmeansvalue.png} % Adjust the width as needed
  \vspace{-1.5\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont The \textit{k}-means elbow is the change in slope of `inertia' (\textit{k}=3)
  and the silhouette is the `score' closest to 1 (\textit{k}=2)}
  \vspace{-0.5\baselineskip} % Remove space after title
  \label{fig:kmeansvalue}
\end{wrapfigure}

\textbf{Classification method 2 - Random forest}  
Including all of the features in the analysis gave an accuracy marginally better 
than achieved using \textit{k}nn. 
I contrast to \textit{k}nn, no performance improvement was found using fewer features, indicating that 
considerably less implementation effort is needed to achieve good performance using random forest. 
A marginal improvement in performance was apparent when island, flipper length or body mass were not included in training.

\textbf{Unsupervised method - \textit{k}-means}  
Although an unsupervised clustering method, 
\textit{k}-means can be used for classification by matching clusters to classes. 
The \textit{k}-means method is normally applied only to numerical features so only they were included in this work. 
The number of clusters (\textit{k}) can be selected using elbow and silhouette methods, 
as shown in Figure~\ref{fig:kmeansvalue}. Empirically, accuracy improved significantly when \(k \geq 4\)
as clusters were not reliably formed for all three species for smaller values of \textit{k},  
Figure~\ref{fig:kmeansmap} illustrates the mapping of classes to clusters for two feature dimensions. 
No improvement in accuracy was obtained by reducing the number of features, 
but, in an additional experiment, a set of \textit{k}-means clusters was created for each sex 
and this led to a small improvement in accuracy. 

\begin{wrapfigure}{r}{0.5\textwidth} % {alignment}{width}
  \centering
  \vspace{-1\baselineskip} % Remove space before the table
  \includegraphics[width=0.48\textwidth]{kmeansmap.png} % Adjust the width as needed
  \vspace{-0.5\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont \textit{k}-means clusters mapped to species using majority voting. 
  Class assignments are shown by polygon colour (\textit{k}=10, 50 samples coloured).}
  \vspace{-0.5\baselineskip} % Remove space before title
  \label{fig:kmeansmap}
\end{wrapfigure}

\textbf{A novel combined visualization and analysis (CVA) approach}  
The CVA approach involves visualizing pairwise plots of features 
to identify a sequence of two-dimensional SVM classifiers. 
CVA requires manual effort to understand the nature of the dataset, 
in contrast with `black box' classification approaches that can be applied with limited knowledge 
of the method and little insight into the nature of the data. 
The main drawback of the CVA approach is that it may not always be feasible to extract the necessary visualizations insights 
particularly for large dimensional datasets. 

An application of CVA to the Penguin dataset is illustrated in Figure~\ref{fig:CVA}. 
Figure~\ref{fig:CVA_part_a} shows the relationship between bill depth and flipper length and 
SVM is used to find a suitable `decision boundary' that separates Gentoo from the other two species. 
Figure~\ref{fig:CVA_part_b} then shows a second SVM line that best separates Adelie and Chinstrap using bill length and bill depth. 
CVA was able to produce results of accuracy almost as good as conventional approaches and a small improvement 
was apparent when separate SVM models were developed for each penguin sex.

\begin{figure}[!ht]
    \vspace{-0.5\baselineskip} % Remove space before figure
    \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{sup_fliplen_billdepth.png} % Adjust the width as needed
    \caption{Gentoo can be distingushed from other species}
    \vspace{-2.2\baselineskip} % Remove space before title
    \label{fig:CVA_part_a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{sup_billlen_billdepth.png} % Adjust the width as needed
    \caption{Adelie and Chinstrap can be partially separated}
    \vspace{-2.2\baselineskip} % Remove space before title
    \label{fig:CVA_part_b}
  \end{subfigure}
  \vspace{2\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont Two-stage CVA approach with boundaries fitted using SVM 
  to training data of feature pairs}
  \label{fig:CVA}
  \vspace{-1.5\baselineskip} % Remove space after title
\end{figure}
% \vspace{\baselineskip}
\subsection*{Conclusions}

With careful data preparation, optimization of metaparameters and robust application of training and testing methods, 
the \textit{k}nn and random forest classification methods produced high-quality results. 
As expected, the \textit{k}-means accuracy results were comparitively worse
as it does not take advantage of target data information known to the supervised approaches. 

The novel CVA approach was able to produce accuracy results
similar to those of other classification methods. 
Although needing to be tailored to each problem and not well-suited to high-dimensionality data, 
its internal operations are transparent and this is in contrast with general-purpose classification methods. 

A classifier that is able to achieve 100\% accuracy for the given data is possible, 
but is likely exhibit poor generalization.

\printbibliography


\end{document}
