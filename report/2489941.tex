\documentclass[a4paper, 11pt]{article}
\usepackage{geometry}
\usepackage{amsfonts, epsfig}
\usepackage{amsmath}
\usepackage{wrapfig} % for wrapping text around figures and tables
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{booktabs} % for better table rules
\usepackage{caption} % to customize caption format
\usepackage{linegoal} % for determining the remaining width of the line
\usepackage[backend=biber, sorting=none]{biblatex}
\usepackage{subcaption}
%\usepackage{hyperref}

% reference file
\addbibresource{2489941.bib}

% custom page style
\fancypagestyle{nofooter}{
  \fancyhf{}
  \fancyhead[L]{Student Number: 2489941}
  \fancyhead[C]{Introduction to AI}
  \fancyhead[R]{\thepage}% 
  \renewcommand{\headrulewidth}{0.4pt}
}
% apply custom page style
\pagestyle{nofooter}

% redefine the table and figure formats
\captionsetup[table]{labelfont=bf}
\captionsetup[figure]{labelfont=bf, justification=centering}
\captionsetup{font=small}

\begin{document}

\begin{center}
  \subsection*{Q1 - Visualization and analysis of the Palmer penguin dataset}
\end{center}

\noindent
The Palmer penguin dataset consists of 344 records of the physical attributes of three species of penguin 
living on three islands in Antarctica (Table~\ref{tab:dataset}) \cite{PM}. In this
report, the
\begin{wraptable}{r}{0.75\textwidth} % {alignment}{width}
  \small
  \begin{center}
  \vspace{-1.7\baselineskip} % Remove space before the table
  \setlength{\abovecaptionskip}{5pt}
  \setlength{\belowcaptionskip}{5pt}
  \fontsize{10}{10}\selectfont % Change font size here
  \begin{tabular}{l|l|l|l}
  % \begin{tabular}{|p{1.8cm}|p{1.2cm}|p{4cm}|p{1.8cm}|}
  \textbf{Feature}&\textbf{Type}&\textbf{Values in the dataset}&\textbf{Importance}\\
  \hline
  island&categorial&Torgersen, Biscoe, Dream&\ \ \ 0.12 (4)\\
  bill length&numerical&32.1mm - 59.6mm&\ \ \ 0.37 (1)\\
  bill depth&numerical&13.1mm - 21.5mm&\ \ \ 0.17 (3)\\
  flipper length&numerical&172mm - 231mm&\ \ \ 0.23 (2)\\
  body mass&numerical&2700g - 6300g&\ \ \ 0.11 (5)\\
  sex&categorial&Male, Female&\ \ \ 0.01 (6)\\
  species&categorial&Adelie, Chinstrap, Gentoo&\ \ \ \ class\\
  \end{tabular}
  \vspace{-2\baselineskip} % Remove space before title
  \end{center} 
  \caption{\centering\linespread{0.8}\selectfont Palmer penguin dataset features. Importance was calculated using random forest and a ranking is shown.}
  \vspace{-2\baselineskip} % Remove space after the table
  \label{tab:dataset}
\end{wraptable}
dataset is cleaned, explored through visualization,  carefully prepared
for investigation and finally the performances of AI approaches in classifying penguin species are compared.


\vspace{\baselineskip}
\subsection*{Data cleaning}

Eleven records have missing values. 
The two missing both \textit{sex} and all numerical features were removed as imputation is unlikely to be reliable. 
A further nine records are missing only a \textit{sex} value and were considered for imputation. 
Figure~\ref{fig:sex} shows the physical 
attributes of the male and female of each species differ statistically, making it likely that imputing \textit{sex} would be reliable. Following standardization, 
a Shapiro-Wilk test confirmed all the numerical attributes have a normal distribution \cite{shapiro1965analysis} 
and Z-tests were applied to assess the 
hypotheses that the missing \textit{sex} value is male or female \cite{freedman2007statistics}. 
Two of the records could be imputed as male and three as female; the 
remaining four records were removed. 
The cleaned dataset has 338 records: 147 Adelie  
(74 male, 73 female), 68 Chinstrap (34 male, 34 female) and 123 Gentoo (62 male, 61 female).

\begin{figure}[!ht]
  \vspace{-0.5\baselineskip} % Remove space before figure
  \begin{minipage}{0.5\textwidth}
    \vspace{0\baselineskip} % Remove space before figure
    \centering
    \includegraphics[width=1\textwidth]{sex.png} % Adjust the width as needed
    \vspace{-1.5\baselineskip} % Remove space before title
    \caption{All numerical features show a significant statistical difference between sexes, for example, the body mass shown here}
    \vspace{-1\baselineskip} % Remove space after title
    \label{fig:sex}
  \end{minipage}\hfill
  \begin{minipage}{0.5\textwidth}
    \vspace{0\baselineskip} % Remove space before figure
    \centering
    \includegraphics[width=1\textwidth]{islands.png} % Adjust the width as needed
    \vspace{-1.5\baselineskip} % Remove space before title
    \caption{Adelie is found on all three islands, but Gentoo and Chinstrap samples are found on only one of the islands}
    \vspace{-1\baselineskip} % Remove space after title
    \label{fig:islands}
  \end{minipage}
\end{figure}


% \vspace{\baselineskip}
\subsection*{Visualization of the dataset}

Figure~\ref{fig:islands} shows Chinstrap and Gentoo penguins are found only on one \textit{island}, making it a potential confounding factor since 
environmental factors may influence physical characteristics. A Shapiro-Wilk test showed  
the numerical features of the Adelie penguins have a normal distribution and an ANOVA test confirmed Adelie features are not 
significantly influenced by the island inhabited. Thus \textit{island} is unlikely to be a confounding factor.

Pairwise scatterplots for the numerical features are shown in Figure~\ref{fig:pairwise}. 
\textit{Bill depth}, combined with either \textit{flipper length} or \textit{body mass}, 
yields a separable cluster of Gentoo penguins (shown in green) allowing them to be identified. 
No pairwise combination completely separates Adelie (orange) from Chinstrap (purple) clusters, 
but the best candidate feature for doing so is \textit{bill length}.

\begin{wrapfigure}{r}{0.58\textwidth} % {alignment}{width}
  \centering
  \vspace{-1\baselineskip} % Remove space before the figure
  \includegraphics[width=0.58\textwidth]{pairwise.png} % Adjust the width as needed
  \vspace{-1.5\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont Pairwise distributions of numerical features. Gentoo can be distinguished, 
  but Adelie and Chinstrap may not be completely separable from one another}
  \vspace{-2.5\baselineskip} % Remove space after title
  \label{fig:pairwise}
\end{wrapfigure}

Figure~\ref{fig:sex} above shows the normative body masses of the male and female differ for all the species. 
Differences between the sexes for the other three numerical physical characteristics were also apparent. 
Since narrower distributions are extracted if the \textit{sex} of the species is considered rather than just the species itself, 
\textit{sex} is likely to provide a finer grained distinction for species classification. 
This knowledge was able to improve performance in some cases, as discussed in `results and analysis'. 

\subsection*{Methodology}

The categorical features in the dataset were encoded to numerical values as this is required for a number of the AI methods. 
Some AI methods are known to be biased in favour of numerical features 
with smaller standard deviations \cite{hastie2009elements}, but this bias can be reduced by the standardization 
of features to zero mean and unity standard deviation. Standardization statistics were calculated only from 
training sets, but the standardization itself was applied to all data, including the test set. 
If a dataset is imbalanced, AI predictions may be biased towards classes more frequently found in the training data. 
All the methods adopted in the current work are known to be little affected by 
imbalanced data \cite{he2009learning}, so no modifications were made.

Predicting the penguin species from the given features is a classification problem. 
The implementation code is available on Github \cite{TimAIRepo} and was written in Python 3.11 \cite{python311} 
using \texttt{scikit-learn} libraries \cite{scikit-learn}.
Results are obtained from two conventional classification approaches, 
namely \textit{k}-Nearest Neighbour (\textit{k}nn) \cite{bishop2006pattern} and random forest \cite{breiman2001random}.
In addition, classificaton was carried out using unsupervised \textit{k}-means (following cluster labelling) \cite{tan2005introduction} 
and a novel combined visualization and analysis (CVA) approach that is a mix of 
practical visualizations and Support Vector Machine (SVM) classification.

\begin{wraptable}{r}{0.6\textwidth} % {alignment}{width}
  \small
  \begin{center}
  \vspace{-0.5\baselineskip} % Remove space before the table
  \setlength{\abovecaptionskip}{5pt}
  \setlength{\belowcaptionskip}{5pt}
  \fontsize{10}{10}\selectfont % Change font size here
  \begin{tabular}{l|l|l}
  \textbf{Method} & \textbf{Metaparameters} & \textbf{Values considered}\\
  \hline
  \multirow{3}{*}{\textit{k}nn}    & nearest neighbours \textit{k}	  & \textit{\textbf{1}}, 2, 3, 4, 5, 6, 8, 10\\
                                   & prediction function	& \textit{\textbf{uniform}},distance \\
                                   & distance metric 	& \textit{\textbf{Manhattan}},Euclid \\
  \hline
  \multirow{6}{*}[0.5ex]{\begin{tabular}[t]{@{}l@{}}random \\ forest\end{tabular}} 
                                   & number of trees & 5, \textit{\textbf{10}}, 15, 20, 25 \\
                                   & maximum depth & \textit{\textbf{no max}}, 10, 20 \\
                                   & min samples to split  & \textit{\textbf{2}}, 5, 10 \\
                                   & min samples at leaf & \textit{\textbf{1}}, 2, 4 \\
                                   & split function & \textit{\textbf{gini}},entropy \\

  \hline
  \multirow{4}{*}{\textit{k}-means} & number of clusters \textit{k}  & 2, 3, \textit{\textbf{4}}, 5, 6, 7, 8, 10\\
                                    & centroid initialize         & \textit{\textbf{k-means++}},random\\
                                    & runs for centroid	     & 2, \textit{\textbf{5}}, 10, 20 \\
                                    & maximum iterations           & 5, \textit{\textbf{10}}, 20, 50 \\
  \hline
    \multirow{3}{*}{CVA}            & regularization   & 0.1, 1, \textit{\textbf{10}}, 100 \\
                                    & kernel coefficient      & \textit{\textbf{1}}, 0.1, 0.01, 0.001 \\
                                    & kernel type                  & rbf, \textit{\textbf{linear}},poly \\
  \hline
  \end{tabular}
  \vspace{-2\baselineskip} % Remove space before title
  \end{center} 
  \caption{\centering\linespread{0.8}\selectfont Metaparameters values shown in italics most consistently produced training results of 
  best accuracy during validation and were selected for generating results}
  \vspace{-1\baselineskip} % Remove space after the table
  \label{tab:metaparameters}
\end{wraptable} 

To reduce the potential for overfitting, the classification methods were trained using 
`holdout validation', where 80\% of the dataset was used in a five-fold cross-validation 
configuration \cite{james2013introduction}. The remaining 20\% was kept for a test set. For all methods, 
the \texttt{scikit-learn} function \texttt{GridSearchCV} was employed to tune metaparameters \cite{scikit-learn}. 
Table~\ref{tab:metaparameters} shows the values selected for the metaparameter grid. Those giving the best 
performance were selected to generate accuracy results (the percentage of correctly predicted species) 
from the test set. The metrics `precision' and `recall' were also calculated, but as 
these are only relevant if false positives or false negatives (respectively) are of interest, 
they are not included in this report.


\subsection*{Results and analysis}

\begin{wraptable}{r}{0.58\textwidth} % {alignment}{width}
  \small
  \begin{center}
  \vspace{-3.5\baselineskip} % Remove space before the table
  \setlength{\abovecaptionskip}{5pt}
  \setlength{\belowcaptionskip}{5pt}
  \fontsize{10}{10}\selectfont % Change font size here
  \begin{tabular}{l|l}
  \textbf{Method} & \textbf{Accuracy (range)}\\
  \hline
  baseline, Adeleie species & 43.49\% \\
  \hline
  \textit{k}NN, all features & 99.24\% (97.06\%-100\%)\\
  \textit{k}NN, no island &	99.46\% (97.06\%-100\%)\\
  \hline
  random forest, all features	& 98.57\% (95.59\%-100\%)\\
  random forest, no body mass & 98.49\% (92.65\%-100\%)\\
  \hline
  \textit{k}-means, numerical features & 97.03\% (94.12\%-97.06\%)\\
  \textit{k}-means, two sex clusters  & 99.18\% (94.12\%-100\%)\\
  \hline
  CVA, three main features & 98.98\% (95.35\%-100\%)\\
  CVA, separate sex models & 99.25\% (90.91\%-100\%)\\
  \hline
  \end{tabular}
  \vspace{-2\baselineskip} % Remove space before title
  \end{center} 
  \caption{\centering\linespread{0.8}\selectfont Classification accuracy mean value and range for 100 pseudo-random test sets 
  and using the metaparameters identified in Table~\ref{tab:metaparameters}}
  \vspace{-1.2\baselineskip} % Remove space after the table
  \label{tab:results}
\end{wraptable} 

The \texttt{scikit-learn}  
pseudo-random procedure was used for selecting validation and test set values and 100 (indices 1 to 100) of these were used 
both when selecting metaparameters and when deriving accuracy results.
The results in Table~\ref{tab:results} include a baseline to demonstrate the 
performance improvements achieved by the AI methods. In classification, the
baseline method is often simply to select the most frequent class in the observations. Here, 
this is the Adelie penguins, giving an accuracy of 43.49\% (147/338). 

\textbf{\textit{k}nn}  
The accuracy of \textit{k}nn obtained using the dataset could be improved by removing certain features 
(particularly those shown as less important in Table~\ref{tab:dataset}).
% \begin{wrapfigure}{r}{0.46\textwidth} % {alignment}{width}
%   \centering
%   \vspace{-0.5\baselineskip} % Remove space before the figure
%   \includegraphics[width=0.48\textwidth]{kmeansvalue.png} % Adjust the width as needed
%   \vspace{-1.5\baselineskip} % Remove space before title
%   \caption{\centering\linespread{0.8}\selectfont The \textit{k}-means elbow is the change in slope of `inertia' (\textit{k}=3)
%   and the silhouette is the `score' closest to 1 (\textit{k}=2)}
%   \vspace{-1\baselineskip} % Remove space after title
%   \label{fig:kmeansvalue}
% \end{wrapfigure}
The best improvement was found when \textit{island} was omitted and when \textit{k}=3. It appears that \textit{island} did not provide 
any additional information and the larger value of \textit{k} implies better generalization has been achieved.

\textbf{Random forest}  
Including all of the features in the analysis, the accuracy of random forest was marginally worse 
than achieved using \textit{k}nn. 
No performance improvement was found using fewer features, indicating that random forest
may be less influenced by superfluous information in the training data. 

\textbf{\textit{k}-means} Although a clustering method, 
\textit{k}-means can be used for classification by 
\begin{wrapfigure}{r}{0.5\textwidth} % {alignment}{width}
  \centering
  \vspace{-0.8\baselineskip} % Remove space before the table
  \includegraphics[width=0.48\textwidth]{kmeansmap.png} % Adjust the width as needed
  \vspace{-0.5\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont \textit{k}-means clusters mapped to species using majority voting. 
  Mappings are shown by polygon colour (\textit{k}=10, 50 samples coloured).}
  \vspace{-1\baselineskip} % Remove space after title
  \label{fig:kmeansmap}
\end{wrapfigure}
mapping clusters to classes. 
Figure~\ref{fig:kmeansmap} illustrates such a mapping for two feature dimensions.
Using the elbow method, the number of clusters (\textit{k}) was recommended to be two, but three was suggested 
by the silhouette method. Empirically, accuracy improved when \(k \geq 4\),
as otherwise clusters were not reliably formed for all three species.   
Accuracy was also greatly improved by creating a separate set of clusters for each \textit{sex}.

\textbf{CVA}  
requires manual identification of suitable pairs of features for SVM classification. 
In general, for features \textbf{x}, SVM seeks to maximize class separation by defining a decision boundary, given by
\vspace{-0\baselineskip} % remove space before equation
\begin{equation}
\mathbf{w}^T \mathbf{x} = b
\end{equation}
where \textbf{w} is a weight vector perpendicular to the boundary and \textit{b} is the intercept. 
SVM training finds \textbf{w} and \textit{b} to specify an optimal hyperplane (and hence the boundary). 
As CVA is a sequence of two feature problems, each decision boundary is a line.
An application of CVA to the Penguin dataset is given in Figure~\ref{fig:CVA}. 
Figure~\ref{fig:CVA}(a) shows the relationship between \textit{bill depth} and \textit{flipper length}, and
the SVM decision boundary separates Gentoo from the other two species. 
Figure~\ref{fig:CVA}(b) plots \textit{bill length} against \textit{bill depth} and shows the SVM boundary to best
distinguish Adelie from Chinstrap. Accuracy was found to be improved by implementing separate SVM models for each \textit{sex}.

\begin{figure}[!ht]
    \vspace{-0.5\baselineskip} % Remove space before figure
    \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{sup_fliplen_billdepth.png} % Adjust the width as needed
    \vspace{-1.2\baselineskip} % Remove space before title
    \caption{ Gentoo distinguished from the other species}
    \vspace{-1.5\baselineskip} % Remove space before title
    \label{fig:CVA_part_a}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{sup_billlen_billdepth.png} % Adjust the width as needed
    \vspace{-1.2\baselineskip} % Remove space before title
    \caption{ Adelie and Chinstrap partially separated}
    \vspace{-1.5\baselineskip} % Remove space before title
    \label{fig:CVA_part_b}
  \end{subfigure}
  \vspace{1\baselineskip} % Remove space before title
  \caption{\centering\linespread{0.8}\selectfont Two-stage CVA approach with decision boundaries fitted to feature pairs using SVM}
  \label{fig:CVA}
  \vspace{-2\baselineskip} % Remove space after title
\end{figure}
\vspace{\baselineskip}

\subsection*{Conclusions}

With careful data preparation, selection of metaparameters and robust application of training and testing methods, 
the \textit{k}nn and random forest classification methods produced high-quality results. 
\textit{k}-means training does not take advantage of target information and this resulted in comparatively poor accuracy, yet training
separate clusters for each sex improved performance significantly.
The novel CVA approach needs to be tailored to each problem and is not well-suited to high-dimensional data, but it was found to produce results comparable to those of general-purpose classification methods and has the advantage that 
its internal operations are easy to visualize, understand and explain.

\begin{center}
\subsection*{Question 2 - Ethical challenges and threats in AI}
\end{center}

\subsection*{Racial Bias in Medical Algorithms}

In 2019, a widely used US healthcare algorithm was found to discriminate by prioritising hospital services 
based on historical spending records, resulting in the allocation of relatively less future funding and fewer referrals 
for black patients \cite{Jemielity2019, Ledford2019}. 
Through the application of a series of test data sets, Obermeyer \textit{et al.} \cite{Obermeyer2019} identified this inadvertent bias 
and the team was able to mitigate against it by adjusting the model’s training labels. 

The fact that this third-party assessment and adjustment were possible, demonstrates how exposing a model’s internal operations 
can aid bias identification and removal \cite{Seroussi2020, Winter2023}. 
Ensuring greater transparency of AI models is becoming the subject of legislation, 
for example the 2023 EU AI Act aims to enforce transparency principles by requiring developers to disclose an 
algorithm’s variables, data sources, and selection logic \cite{EuropeanParliament2023, Edwards2021}. 
While ensuring that organisations building AI systems are held accountable for the processes used in their development 
may lead to algorithmic changes that reduce bias \cite{Donovan2018, Lawry2020}, 
care needs to be taken that the removal of bias doesn't significantly affect the performance of the model 
in its application domain \cite{Kearns2020}.

\subsection*{AI system safety and existential risks in warfare}

Recent developments in AI have led many researchers to believe that AI systems capable of directly acting 
in the real world based on decisions they have taken autonomously will become available later this century \cite{Grace2018}. 
With such advancements comes the risk that AI systems whose decision-making does not prioritise human welfare 
may pose a threat to life \cite{Ord2020}. 

A specific example of a military AI system posing an existential risk \cite{yampolskiy2016, Cummings2017}, 
is one that decides maximising human casualties would be the best strategy to achieve a high-level battlefield objective \cite{Barrat2013}. 
Recent deployments of automated missiles that activate on target acquisition \cite{Atherton2021, BodeWatts2023}, 
have raised ethical concerns over the use of AI in situations where human beings are potential targets \cite{Emery2021}. 
If such advanced AI was given control of powerful military weapons and applied more widely, 
the ramifications for the human race's survival could be profound \cite{Tegmark2017}. 

Addressing these existential threats requires international cooperation 
to guarantee the transparency of AI algorithms \cite{Cihon2019, Leslie2019}. 
A potential future safeguard is to include a human-controlled override in all military AI systems \cite{CritchKrueger2020}, 
although Russell \cite{Russell2019} warns that super-intelligent AI may be capable of removing such safety measures. 
Ultimately, a global strategy that prioritises human wellbeing in all areas of AI usage will be essential.

\printbibliography
\end{document}
